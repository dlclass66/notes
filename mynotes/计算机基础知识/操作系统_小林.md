
# 硬件结构

## cpu是如何执行程序的

### 冯诺依曼模型

计算机基本结构为5个部分 运算器，控制器，存储器，输入设备，输出设备

### 内存

基本单位byte字节 1byte = 8 bit（位）

### cpu

32位处理器一次处理4个字节（32位） 64位一次处理64位
32位最大操作4g内存 （2^32 byte）
软件的位宽指指令的位宽
一般64位的电脑可以运行32位的程序 反之则不行
常见寄存器

1. 通用寄存器 用来存放需要计算的数据
2. 程序计数器 存储下一条指令的地址
3. 指令寄存器 存储正在执行的指令

### 总线

用于通信
有以下几种

1. 地址总线 用于指定地址
2. 数据总线 用于读写数据
3. 控制总线 用于发送和接收数据

### cpu执行程序的大致过程

读取程序计数器对应的指令存入指令寄存器， 程序计数器自增（增大多少取决于cpu的位数） cpu执行指令寄存器中的指令 然后继续第一步
数据和指令在内存中是分开存放的 指令放在正文段 数据放在数据段
现代cpu多采用流水线的方式执行指令 一条指令被分为4个阶段 称为4级流水线

1. fetch cpu通过程序计数器读取指令
2. decode 对指令进行解码
3. execution 执行指令
4. store 将计算结果回写寄存器或者将寄存器的值写入内存
这4个阶段称为指令周期

### 指令类型

1. 数据传输
2. 运算
3. 跳转（用来实现条件分支 函数调用等）
4. 信号类型（比如中断trap）
5. 闲置（执行后cpu空转）

### 执行速度

cpu参数有时钟频率 表示一秒cpu能完成的基本动作的数量 但一条指令不一定只对应一个基本动作

## 磁盘与内存

### 存储器结构

离cpu由近到远 速度由快到慢 容量由小到大 寄存器 cpu cache（l1, l2, l3）内存 硬盘
32位cpu中大多数寄存器可以存储4字节 64位中大多数可以存储8字节
l1缓存通常分为指令和数据两个部分 每个核心都有 l2缓存每个核心都有 l3缓存通常多个核心共用
cpu并不会直接和每一种存储设备打交道 每种存储设备只和其相邻的设备交互 相当于层层缓存

## 如何写出让cpu运行更快的代码

对于计算密集型的程序来说 cpu运行效率很重要 让cpu计算所需要的数据能尽可能的在cpu cache中而不是内存中可以提高效率

### cpu cache的数据结构和读取过程

cache line（缓存块）是cpu从内存读取数据的基本单位 由标志（tag） 和数据块（data block）组成
cpu读取数据时 会先访问cache 没有命中再访问内存 这里就存在一个问题 cpu如何确定要访问的数据在不在cache中
最简单是使用直接映射 内存块对应的cache line由取模决定 唯一确定 但是这样可能出现多个内存块映射到同一个cache line上 为了区别不同的内存块 在对应的cache中还会存储一个tag 而cpu从cache中读取数据时 并不是读取整个cache 而是读取数据片段 （字word）这就需要一个偏移量

### 提高数据命中率

连续的访问 可以提高缓存命中率

### 提高指令命中率

分支中有规律的话 cpu会提前预测 这样也可以提高命中率

### 多核

由于l1 l2 不是多核心共享的 所以对于计算密集型 为了让缓存命中率更高 最好限制在一个核心上运行

## cpu缓存一致性

### cpu cache的数据写入

数据写入cache 导致内存与对应的cache中的数据不同 所以必须要把cache中的数据同步到内存中
有以下两种方法

1. 写直达（write through）
2. 写回（write back）

#### 写直达

最简单的方法 把数据同时写入内存和cache中
写入前如果数据已经在cache中了 就先更新cache 再写到内存中
如果没有在cache中 就直接把数据更新到内存里
直观 简单 但问题是每次操作都要写回到内存 性能受影响

#### 写回

当发生写操作时，新的数据仅被写入cache block中 只有当被修改过的cache被替换时才需要写到内存中 减少了数据写回内存的频率
具体做法

* 当cpu发生写操作时 如果cache里面有需要修改的数据 则修改 然后标记为脏
* 如果对应的cache存储的是其他内存块 这个时候就需要替换了 如果是脏的就写回内存 否则就不需要 然后把要写入的数据读取到cache中 修改并标记为脏

### 缓存一致性

如果使用写回策略 多核心操作同一变量就会出现问题 比如一个核心执行了i++ 只更新了cache而没有写回内存 另一个核心读取到的i的值就是错误的
要解决这一问题就需要同步不同核心的缓存 需要以下两点

1. 某个核心的cache数据更新需要传播的其他核心（写传播 write propagation）
2. 某个核心对数据的操作顺序，必须在其他核心看起来一样 事务串行化（transaction serialization） 比如核心a和b同时更新了i 传播到cd核心 而cd中ab更新顺序不同 就会产生错误
所以第二点需要锁 如果两个核心的cache有相同数据 那么对于这个相同数据的更新 需要拿到锁才能进行

### 总线嗅探

为了实现写传播 某个核心更新了cache需要广播 每个核心都会监听总线上的广播事件 如果有对应数据在自己的cache中 就更新 这种方法很简单 但是会加重总线的负担 也无法实现事务串行化

### mesi协议

基于总线嗅探机制 实现了事务串行化 用状态机制降低了总线带宽压力 做到了缓存一致性
用4个状态标记了cache line

1. 已修改 就是脏数据 修改也无需广播
2. 独占 干净数据 并且只有一个cache中有 可以自由操作 不需要通知其他核心 其他核心从内存中也读取了同样的数据就变成共享
3. 共享 干净数据 在多个cache中有 修改时需要广播 把其他cache中的相同数据设置为无效 然后再更新 变为已修改
4. 无效 数据已经失效 不可读取
只有修改3状态的数据才需要广播 减低了总线负担
运用了有限状态机的思想

## cpu是如何执行任务的

### catch伪共享

多个线程同时读写同一个cache line的不同变量，导致cpu cache失效的现象 （cache line会被标记为共享）性能受到影响

### 避免方法

对于多个线程共享的热点数据 即经常修改的数据 要避免恰好在一个cache line中
linux中有一个宏（__cacheline_aligned_in_smp）用于解决伪共享问题 在多核cpu中可以让变量对齐cache line 使其不会出现在同一个cache line中 这样就用空间换了时间
应用层也有解决办法 可以用一些空白填充不会被修改的数据的前后

### cpu是如何选择线程的

在linux中 进程和线程都是用 task_struct 结构体表示的 区别在于 线程的结构体内的一些资源是共享进程已经创建的资源 如代码段等 所以线程也被称为轻量级进程
线程进程统称为任务
linux中根据任务优先级及响应要求 主要分为两种 优先级数值越小 优先度越高

1. 实时任务 要尽可能快的执行 优先级0 ~ 99
2. 普通任务 没有很高要求 100 ~ 139

#### 调度类

为了保障高优先级的任务能够尽可能早的执行

#### 完全公平调度

我们遇到的基本都是普通任务 公平性很重要 linux中有一个基于cfs（完全公平调度）的调度算法
它为每一个任务安排一个虚拟运行时间 在运行的程序 运行得越久 虚拟时间就越大 没有运行的 虚拟运行时间不变 调度的时候会优先选择虚拟运行时间小的 考虑到 优先级 还需要加权 让优先级高的虚拟运行时间短

#### cpu运行队列

每个核心都有自己的任务队列
完全公平调度对应的队列是用红黑树实现的 按虚拟运行时间排序

#### 调整优先级

启动任务 默认是普通任务 可以通过调整nice值调整优先级 但是仍然是普通任务 如果要进一步调整 就要改变任务的优先级和调度策略 使其变为实时任务

## 软中断

### 中断是什么

系统响应硬件设备请求的一种机制 操作系统收到请求后 会打断正在执行的程序 调用中断处理程序响应请求
中断是一种异步事件处理机制 可以提高系统的并发能力 中断处理程序要尽可能快的执行完 减少对正常进程运行调度的影响 而且 中断处理程序响应中断时 可能会临时关闭中断 这时系统的其他中断请求都无法被响应 可能会丢失中断

#### linux软中断

linux为了解决中断处理程序执行过长和中断丢失的问题 将中断过程分为了两个阶段

1. 上半部分快速处理中断 一般会暂时关闭中断请求 主要负责处理跟硬件紧密相关或者时间敏感的事情
2. 下半部分延迟处理上半部为完成的工作 以内核线程的方式运行

也可以理解为

1. 上半部直接处理硬件请求 也就是硬中断 主要负责耗时短的 特点是快速执行
2. 由内核触发 软中断 负责上半部未完成的工作 耗时较长 延迟执行
所以硬中断会打断cpu正在执行的任务 然后立即执行 而软中断是以内核线程的方式运行 每一个cpu核心对应一个软中断内核线程 一些内核自定义事件也属于软中断

#### 系统中的软中断

软中断是以内核线程的方式运行的 所以用ps命令（显示当前运行进程的状态 类似windows的任务管理器）可以看到  内核线程的名字外面都有中括号 因为ps无法获取他们的命令行参数 所以一般来说 名字在中括号里面的都可以认为是内核线程

#### 如何定位软中断cpu使用率过高的问题

要想知道当前的系统的软中断情况 我们可以使用top命令查看 si代表软中断 可以得到cpu软中断的使用率 根据不同类型软中断的中断次数变化速率可以确定哪种软中断占比最高 比如web服务器就可以分析哪个网卡对应的中断最多 再抓包分析

## 负数 小数的表示

### 负数用补码表示

int 32位 最高位是符号标志位 0表示正数 1表示负数 剩下31位表示2进制数据
负数是以补码表示的 就是把其对应的正数（绝对值）二进制全部取反再加1
使用补码的方式在进行加减法时 直接用二进制的规则即可 提高了运算的效率 所以补码就相当于用0 减去其绝对值

### 小数用浮点数

一种表示方法是使用定点数 小数点后第一个二进制位表示2的-1次方 但是有很多小数无非被这样表示
计算机采用浮点数的方式（类似科学计数法）比如1000.101（二进制定点数）写为浮点数就是 1.000101 * 2 ^ 3 最关键的是000101和3 000101称为尾数 3称为指数 然后还需要一个符号位
符号位表示正负 0为正 1为负 指数位长度决定了数值的表达范围 尾数位长度决定了数值的精度
32位的浮点数为单精度 float 64位的是双精度 double
指数在存储过程中会加上一个正的偏移量 用无符号整数存储
因为有的十进制小数无法转换为精确的二进制定点数 所以小数在计算机中是近似的

# 操作系统结构

## linux内核和windows内核

### 内核

内核是应用和硬件的中间层 让应用只需要和内核交互 而无需关心硬件 降低了开发难度
现代操作系统 内核一般会提供4大基本能力

1. 进程调度 决定哪个进程使用cpu
2. 内存管理 决定内存的分配与回收
3. 管理硬件 为进程与硬件间提供通讯能力
4. 提供系统调用 用户程序通过这个方式与系统交互

为了安全 内核的权限很高 可以控制硬件 而应用程序权限小 内存被分为两个区域 内核空间（只有内核才有权访问）和用户空间（专门给用户程序使用）
在内核空间称为内核态 在用户空间称为用户态 应用程序如果需要进入内核态 就需要系统调用
过程 应用程序使用系统调用时 产生中断 cpu运行中断处理程序 也就是开始执行内核处理程序 处理完后 主动触发中断 回到用户态

### linux

基于c语言 开源
内核设计理念主要有以下几点

1. multitask 多任务
2. smp 对称多处理
3. elf 可执行文件链接格式
4. monolithic kernel 宏内核

#### 多任务

多任务表示多个任务可以并发或并行执行 每个任务执行一小段时间就切换到另一个任务 宏观来看一段时间执行了多个任务 这叫并发
多个任务被不同cpu同时执行 这叫并行

#### smp 对称多处理

代表每个cpu核心地位相等 每个核心都可以访问完整的硬件资源
每个程序都可以被分配到任意一个cpu核心上执行

#### elf

是linux中可执行文件的存储格式 elf将文件分段
elf的生成
我们编写的代码 首先通过编译器变成汇编代码 然后通过汇编器变成目标代码 最后通过链接器 把多个目标文件及调用的各种函数库链接起来 形成elf文件
elf的执行
通过装载器把elf装载到内存中 然后取指执行

#### 宏内核

linux内核是一个完整的可执行程序 并且拥有最高权限
特征 系统内核所有模块 如进程调度 内存管理等都运行在内核态
但是linux也实现了动态加载内核模块的功能 比如大部分驱动就是动态加载的 与内核其他模块解耦 让驱动开发与加载更为方便灵活
与其相反的是微内核
内核只保留最基本的能力 而其他的被放到了用户空间 比如驱动程序 文件系统 这样服务与服务之间是隔离的 单个服务出现故障不会导致整个操作系统崩溃 提高了操作系统的稳定性和可靠性
微内核功能少 可移植性高 但是不好的地方在于 由于驱动不在内核中 而驱动需要频繁与硬件交互 这就需要频繁切换到内核态 带来了系统损耗 鸿蒙就是微内核
还有一种是混合内核 架构有点像宏内核包裹着一个微内核

### windows

Windows的内核叫Windows nt
windows和linux一样 也支持multitask和smp 但是不同的是 windows的内核设计是混合型的 可执行文件的格式也不同 Windows的可执行文件格式叫pe 扩展名通常是.exe .dll .sys等

# 内存管理

## 为什么需要虚拟内存

单片机没有操作系统 cpu直接操纵内存物理地址 很麻烦 还可能互相冲突
操作系统把进程所使用的地址隔离开 每个进程有自己的虚拟地址空间 而虚拟地址到物理地址的映射由操作系统完成（cpu的mmu芯片） 对应用透明
有两种管理方式 内存分段和分页

### 内存分段

分段较早提出 程序是由若干个逻辑分段组成 代码分段 数据分段 栈段 堆段  
分段机制下 虚拟地址由两部分组成 段选择因子和段内偏移量
段选择子保存在段寄存器里面 最重要的是段号 用作段表的索引 段表里面保存了段的基地址 段的界限和特权等级
段内偏移量 如果段内偏移量是合法的 物理内存地址就是段基址加上偏移量  
分段的不足之处

1. 内存碎片
2. 内存交换效率低

#### 内存碎片

可分为内部碎片和外部碎片
内存分段管理可以根据段实际需求分配内存 所以不会有内部碎片 但是由于段的长度不固定 会产生多个不连续的小物理内存 出现外部碎片
可以使用内存交换解决外部碎片问题 linux就有swap分区 可以通过把内存写入swap分区 再写回内存 以更合理的方式排列 但是对与多进程的系统来说 外部碎片很容易产生 而swap需要用到硬盘 存在很大性能瓶颈

### 内存分页

可以少出现一些碎片 当需要内存交换时 交换的数据也更少一点
分页是把整个虚拟和物理内存空间切成一段段固定大小的空间 被称为页 linux下页的大小为4kb
虚拟地址和物理地址通过页表来映射 页表存储在内存中 mmu(内存管理单元)做地址转换的工作
当进程访问的虚拟地址在页表中查不到时 系统会产生一个缺页异常 进入内核态分配物理内存 更新进程页表 在回到用户态 恢复进程运行
分页是如何解决外部碎片和内存效率低的问题的
分页时 每个页固定大小 相邻紧密排列 不会有外部碎片
但是会有内存浪费的现象（内部碎片）因为进程使用的物理内存的最小单位是页  
当内存空间不够时 操作系统会把内存中最近没被使用的页暂时写到磁盘（换出）需要的时候再加载到内存（换入）这样就减少了需要内存交换的数据量 提高了内存交换的效率
加载程序时 我们也不需要一次就把程序加载到内存中 只加载需要用到的虚拟内存页即可
虚拟地址和物理地址的映射
虚拟地址分为两部分 页号和页内偏移 页号是页表的索引 页表包含了每页在物理内存的基地址 基地址和页内偏移就形成了物理地址
简单分页的缺陷
主要是空间问题 操作系统可以同时运行非常多的进程 假设32位环境下 虚拟地址空间共4g 而一个页的大小是4k 假设每个页表项要4个字节 则4g的空间需要4m内存存储页表 而每个进程极限情况下都需要这么大的空间存储页表

#### 多级页表

可以把页表项也进行分页 比如二级页表的虚拟地址包括 一级页号 二级页号 页内偏移 通过一级页号到对应的页表寄存器中得到二级页表地址
通过二级页号和二级页表得到 最终的物理内存页地址
如果4g虚拟地址全部映射到物理内存上 二级分页占用的空间其实更大 但是我们往往不会分配那么多空间 然后如果某个一级页表的页表项没有被用到 就不需要创建其对应的二级页表了  
分页的页表必须覆盖全部的虚拟内存地址 （因为虚拟地址在页表中找不到对应的项，系统就无法工作）单级页表必须给每个分页一个页表项 而多级分页靠第一级分页就可以覆盖整个虚拟内存空间了
64位系统 需要4级目录

#### tlb

多级页表解决了空间上的问题 但是由于增加了地址转换的流程 也就增加了时间上的开销
由于程序具有局部性 所以可以把最常访问的几个页表存储到访问速度更快的硬件 cpu中有一个专门存放程序最常访问的页表项的cache 也就是tlb 通常被称为页表缓存 转址旁路缓存 快表等
tlb的命中率其实很高

### 段页式内存管理

分页和分段并不是对立的 可以组合起来在同一个系统中使用 被称为段页式内存管理
实现方式

1. 先将程序划分成多个有逻辑意义的段  
2. 再将每个段划分成页

这样 地址结构就由段号 段内页号 页内偏移组成
每个程序有一个段表 每个段有一个页表 段表中的地址是页表的地址 页表的中存储了每个页号对应的物理页号
要得到物理地址需要三次内存访问

1. 访问段表
2. 访问页表
3. 根据物理页号和页内偏移访问物理地址

可用软硬件结合的方式实现段页式地址变换 虽然增加了硬件成本和系统开销 但是提高了内存利用率

### linux内存布局

#### intel处理器的内存管理

早期使用段式内存管理 后实现了页式内存管理 但是页式内存管理的作用是在由段式内存管理所映射的地址上在加上一层地址映射  
程序使用逻辑地址 通过段内存管理变为（线性地址）虚拟地址 然后通过页内存管理映射为物理地址

#### linux的内存管理

主要采用页式 也涉及了段式
因为intelx86cpu 会先段式映射 但是linux中的每个段都是从0地址开始的整个4g虚拟空间（32位）也就屏蔽了处理器中的逻辑地址概念 段只用于访问控制和内存保护

#### linux虚拟地址空间的分布

被分为内核空间和用户空间 一般内核空间占据高位 在用户态只能访问用户空间 内核态才可以访问内核空间  
虽然每个进程有自己独立的虚拟内存 但是其内核地址关联的都是相同的物理内存 这样可以让进程切换到内核态后 更方便的访问内核空间内存

#### 用户空间分布情况

以32位为例 从低到高分别是六种不同的内存段

1. 代码段 包括二进制可执行代码
2. 数据段 包括已初始化的静态变量和全局变量
3. bss段 未初始化的静态变量和全局变量  
4. 堆段 动态分配的内存 从低开始向上增长
5. 文件映射段 包括动态库 共享内存等 从低地址向上增长 （跟硬件和内核版本有关）
6. 栈段 包括局部变量和函数调用的上下文 栈的大小是固定的 一般为8mb 当然也可以自定  

代码段下还有一段保留区 因为在大多数系统中 较小数值的地址被认为不合法 比如在c代码中 无效的指针会被赋值为NULL 有了不可访问的内存保留区 就可以处理这些情况
在这7个内存段中 堆和文件映射段是动态分配的 malloc（）可以在堆动态分配内存 mmap（）可以在文件映射段动态分配内存

### 虚拟内存的作用

1. 进程可以运行 即使其总内存超过物理内存大小 通过swap
2. 解决了多进程之间的地址冲突
3. 页表中的页表项除了物理地址外 还可以标记页属性 提供了更好的安全性

## malloc 是如何分配内存的

malloc（）并不是系统调用 而是c库中的函数
malloc申请内存时 有两种方式向操作系统申请堆内存

1. 通过brk（）系统调用 （堆）
2. 通过mmap（）系统调用 （文件映射区）

方式一实现很简单 就是将堆顶指针向上移动（高地址）
方法二 通过mmap（）中的私有匿名映射方式 在文件映射区 分配了一片内存
对于小内存 一般用brk（）（方式一） 大的用mmap（）（方式二）

### malloc分配的是物理内存吗

不是 malloc分配的是虚拟内存 如果分配后的虚拟内存没有被访问的话 虚拟内存就不会映射到物理内存 这样就不会占用物理内存
访问已分配的虚拟内存地址时 操作系统通过查找页表 发现虚拟内存对应的页没有在物理内存中 就会触发缺页中断 然后操作系统会建立虚拟内存和物理内存之间的映射关系

### malloc（1）会分配多大的虚拟内存

malloc分配空间时 并不会老老实实按用户要求分配 而是会预分配更大的空间作为内存池 具体分配多大与malloc使用的内存管理器有关

### free释放内存 会归还给操作系统吗

free释放内存后 堆内存还是存在 并不会归还给操作系统 因为与其把内存释放 还给操作系统 不如先缓存着 放入malloc的内存池中 当进程再次申请时就可以直接复用  
当进程退出后 操作系统才会回收进程的所有资源  
上面所说的针对brk（）方法得到的内存 如果是mmap方法 free释放后就会直接还给操作系统

### 为什么不全部使用mmap

因为分配内存需要系统调用 用户态和内核态的切换会消耗不少资源 另外 因为mmap每次释放内存时都会归还给操作系统 所以每次mmap分配的虚拟内存都是缺页状态 第一次访问都会触发缺页中断 这些都会造成很大的cpu消耗 而brk方法有内存池 减少了系统调用和缺页中断的次数

### 为什么不全部使用brk

因为brk不会马上释放 所以频繁的malloc和free会产生越来越多的内存碎片 （内存泄漏） 而且很难检查出来
所以brk和mmap要配合使用

### free只传入了内存地址 为什么可以知道要多大的内存

malloc给用户态的内存起始地址比堆空间起始地址多了16字节（相当于有16字节并不能被用户使用） 这16字节保存了该内存块的描述信息 比如该内存块的大小
当执行free函数时 free会对传进来的地址偏移16个字节 从这16个字节中得到内存块的大小  

## 内存满了 会发生什么

### 内存分配的过程是怎样的

1. malloc申请到虚拟内存
2. 应用程序读写到该内存时 产生缺页中断
3. 缺页中断处理函数查看是否有空闲的物理内存 有的话就直接分配 建立映射关系  
4. 没有的话 内核就进行内存回收 主要有两种方式 直接内存回收和后台内存回收

后台内存回收（kswapd）在物理内存紧张时 唤醒kswapd内核线程回收内存 这个过程是异步的
直接回收内存（direct reclaim）如果后台异步回收跟不上进程内存申请的速度 就会直接回收 这个过程是同步的
如果直接回收内存后 空闲的物理内存仍然无法满足要求 内核就会触发oom机制
oom killer机制会根据算法选择一个内存占用较高的进程 杀掉 如果还不够就继续选择然后杀掉 直到足够

### 哪些内存可以被回收

主要有两类 回收方式也不同

1. 文件页 内核缓存的磁盘数据和文件数据叫文件页 大部分文件页 都可以直接释放内存 有需要时 再从硬盘重新读取就可以 而脏页就得先写入硬盘 才能进行内存释放  
2. 这部分内存没有实际载体 不像文件缓存有硬盘文件这个载体 比如堆，栈数据等 这部分内存很可能还会被再次访问 所以不能直接释放内存 这类文件回收方式是通过swap  

文件页和匿名页的回收都是基于lru算法 也就是优先回收不常访问的内存 lru算法实际维护着 active和inactive 两个双向链表
active_list 活跃内存页链表 存放最近被访问过的（活跃）的内存页
inactive_list 不活跃内存页链表 存放的是很少被访问的（非活跃）的内存页
越接近链表尾部 表示内存页越不常被访问 这样在回收内存时 系统就可以优先回收不活跃的内存  

### 回收内存带来的性能影响

回收内存基本都会发生磁盘i/o 势必会影响系统性能  
下面是一些常见解决方案

#### 调整文件页和匿名页的回收倾向

文件页的影响会相对较少 因为干净页不需要写回硬盘 而匿名页swap换入换出都会发生磁盘i/o
linux提供了一个选项 可以调整文件页和匿名页的回收倾向 可以尽量回收文件页

#### 尽早触发kswapd内核线程异步回收内存

系统抖动 而且抖动时应用程序每秒直接扫描的页的数值很大 大概率就是因为直接回收内存导致
针对这个问题 解决方法是尽早触发后台内存回收 避免应用程序直接内存回收
触发后台内存回收的条件  
内核设置了三个内存阈值（水位）用来衡量当前剩余内存充裕或者紧张  

1. 页最小阈值
2. 页低阈值
3. 页高阈值

这三个内存阈值会划分为4种内存使用情况
剩余内存大于3 则内存充足 23之间 内存分配正常 12之间 内存压力大 小于1 内存基本耗尽
kswapd会定期扫描内存 根据剩余内存情况来进行回收工作
第12情况都无需操作 第三种情况kswapd0会执行内存回收 直到变为情况1 第4种情况会直接触发直接回收
页最小阈值可以直接设置 页高阈值和页低阈值和其是线性关系 由固定公式计算得出  
为了尽早回收 可以适当增大页最小阈值 但是这样会让可用内存降低

#### numa架构下的内存回收

smp架构 多个cpu共享资源的架构 每个cpu地位平等 也被称为一致存储访问架构 但是随着cpu处理器核数的增多 多个cpu通过一个总线访问内存 总线的带宽压力会越来越大 每个cpu的可用带宽会越来越小  
为了解决这个问题 就研制出numa结构 即非一致存储访问结构  
numa将cpu分组 每一组cpu用node表示 一个node可能包含多个cpu 每个node有自己独立的资源 每个node通过互联模块总线通信 所以每个node上的cpu还是可用访问到整个系统中的所有内存 但是访问远端的node内存比访问本地内存要耗时很多
与内存回收的关系  
当某个node内存不足时 可以从其他node寻找内存 也可以从本地中回收内存 可以控制 有以下几种选项

1. 默认 回收本地内存前 在其他node寻找空闲内存  
2. 只回收本地内存
3. 只回收本地内存 将脏页写回硬盘 回收  
4. 只回收本地内存 采用swap方法  

虽然访问远端耗时多 但是相比内存回收还是很小 一般选择第一种方案

### 如何保护进程不被oom杀掉

选择被杀进程的标准 进程已经使用的物理页面数 越大越容易被杀掉 还有一个可以手动设置的校准值 可以通过修改校准值让一个进程不可能被杀掉 可以将重要的系统服务进程设置为不可被杀掉 最好不要把业务程序设置为不可被杀掉  

## 在4gb物理内存的机器上申请8gb内存

没有确定的答案 需要考虑3个前置条件

1. 操作系统是32位还是64位
2. 申请完8g内存后会不会被使用
3. 操作系统有没有使用swap

### 操作系统虚拟内存大小

malloc申请的是虚拟内存  
在32位系统中 进程最多只能申请3gb的虚拟内存（一共最大4gb 有1g是内核空间） 在申请虚拟内存阶段就会失败 cannot allocate memory
64位 可以分配虚拟内存 但是linux中有一个参数 有三种情况

1. 默认情况 运行申请大于物理内存大小的虚拟内存 但是超过太多就会被拒绝
2. 允许 来者不拒
3. 拒绝

所以如果是默认的情况下 也可能会被拒绝
虽然理论上虚拟内存没被访问就与物理内存无关 但是申请虚拟内存的过程中还是使用了物理内存 所以如果大量申请就有可能触发oom 杀掉进程  
开启swap后 64位系统就可以申请127t内存了

### swap机制的作用

如果申请物理内存的大小超过了空闲物理内存大小 如果开启了swap 可以正常运行 如果没有 就会oom
swap就是把一块磁盘空间当作内存来用 内存不够用了就把一部分数据写到磁盘（换出） 要用的时候再写回到内存（换入）
优点 应用程序实际可用的内存远远超过物理内存 但是频繁的读写磁盘 会显著降低操作系统运行速率
linux的swap会在内存不足和内存闲置的场景下触发

1. 内存不足 需要内存超过可用内存 触发直接回收 内核将不常使用的页交换到硬盘 这个过程是同步的  
2. 内存闲置 应用程序启动阶段会使用大量内存后面不再使用 当空闲内存低于一定水位时 就会启动后台进程（kswapd） 将这些不用的页交换到硬盘 这个过程是异步的  
linux提供了两种不同的方法启用swap 一种是swap分区 一种是swap文件
匿名页回收的方式是swap  
有了swap 进程可用使用的内存也不是无上限的

## 如何避免预读失效和缓存污染的问题

1. 操作系统读磁盘时会多读一些到内存中 但是最后没用到 如何改善
2. 批量读数据 可能会把热点数据挤出去 如何解决

这需要改进lru算法 传统的lru算法存在以下问题

1. 预读失效 导致缓存命中率下降 对应第一个问题
2. 缓存污染导致命中率下降 对应第二个问题

redis的缓存淘汰算法通过实现lfu算法避免缓存污染 redis没有预读
mysql和linux通过改进lru算法避免预读失效和缓存污染

### linux和mysql缓存

linux在应用程序读取文件数据时 对读取的文件数据进行缓存 缓存在文件系统的page cache中（属于内存）
mysql的数据是存储在磁盘中的 为了提高性能 innodb 存储引擎设计了一个缓冲池（属于内存）
修改时 先修改缓存池中的数据 然后设置为脏页 由后台线程将脏页写入磁盘

### 传统lru是如何管理内存数据的

因为缓存区大小是有限的 所以需要淘汰机制 淘汰一些很少访问的数据 同时保证常用数据留在内存中
lru算法一般用链表实现 头部是最近使用的 尾部是最久没被使用的 空间不够时 淘汰尾部即可  
传统的实现思路是这样的 如果访问的页在内存中 就移到头部 如果不在就把该页放到链表头部 再删除末尾的页
传统的实现方式并没有被linux和mysql使用 因为无法避免上文的两个问题

### 预读失效 怎么办

linux为基于page cache的读缓存机制提供了预读机制 磁盘基本读写单位为block（4kb和页一样大） 读取磁盘时 系统会多读几个block放到内存中 由于局部性 这样做可用减少磁盘i/o次数 提高磁盘吞吐量  
mysql也有类似的预读机制
如果预读的页没有被访问 就是预读失效 在传统的lru算法中 预读页会被放在lru链表的头部 如果一直没被访问到 就可能会挤掉热门数据 降低缓存命中率

#### 如何避免预读失效

大部分情况下 局部性有效 所以不能去掉预读 最好的办法是让预读页停留在内存的时间尽可能短 让真正被访问的页才移动到头部  
具体改进措施

1. linux实现了两个lru链表 活跃和非活跃
2. mysql在一个lru链表上划分两个区域 yong和old

这两个解决办法的思路是类似的 都是将数据分为冷数据和热数据 然后分别进行lru算法

在linux中 预读页会被加入到非活跃lru的头部 真正被访问时才加入到活跃lru的头部 如果一直没被访问就会被从非活跃lru中移除 当活跃lru内存不够时 页会被降级为非活跃而不是直接移除 当非活跃lru内存不足时 才会被移除
在mysql中 lru链表被分为两个部分 yong在前段 old在后端 这两个段的长度并不相同 yong：old 63：37（默认）
预读的页加载到 old的头部 真正被访问才插入到young的头部  

### 缓存污染怎么办

上述改进可用避免预读失效 但是如果还是只要数据访问一次就加入到活跃lru头部 缓存污染仍旧无法避免 比如批量读取数据时 由于数据被访问了一次 大量数据被加入到活跃lru 之前的热点数据就会被淘汰 如果这些新的数据很长时间不被访问 整个活跃lru就被污染了

#### 缓存污染的问题

当热数据再次被访问时 由于缓存未命中 产生大量的磁盘i/o 系统性能急剧下降 以mysql为例 当某一sql语句扫描大量数据时 可能将大量热数据淘汰  （这个过程可能并未查询出大量数据）
如何避免缓存污染的影响  
前面的lru算法加入活跃lru的门槛太低（访问一次）解决方案就是提高门槛
linux 在内存页被访问第二次时才从非活跃升级到活跃
mysql 在第二次时不会马上升级 还要进行停留在old区域的时间判断 如果第1次 第二次之间的时间在一秒内（默认） 不会升级 如果超过一秒才会升级
这样前面的批量访问的数据就不会全部进入活跃lru  

## 深入理解linux虚拟内存管理

每一个虚拟空间的字节都有一个虚拟地址与其对应 32位最大8g 64位最大256t（只用了48位 最高128t是内核 最低128t是用户 中间的是空的 内核高16位都是1 用户低16位都是0）
虚拟地址的最大效果就是让程序不再需要关心其它程序 有一种所有内存都归自己使用的错觉
用户空间的虚拟内存分布

1. 代码段 存放二进制机器指令
2. 数据段 存放已经初始化的变量
3. bss段 存放未初始化的数据 这些数据加载进内存是 默认是0 存在bss段 bss段大小固定
4. 堆 程序运行过程中动态申请的内存
5. 文件映射与匿名映射区 存放动态链接库等 还有mmap
6. 栈 存放函数调用的局部变量和函数参数

在代码段下面还有保留区 可以用来作为null的地址 在64位中 代码段和数据段中间还有不可访问区 避免越界
子进程在新创建出来之后他的虚拟内存空间和父进程一样 因为是拷贝过来的（通过fork）
而通过vfork或者clone调用创建出来的进程 并不会拷贝 而是让父子共享 这就是子线程 这样做可以减少系统切换开销

task_struct 是管理进程的结构 而mm_struct是其中专门管理虚拟内存地址的结构 都保存在内核空间中
内核空间地址由低到高分别是

1. dma映射区 直接映射到物理内存 供dma相关使用
2. normal映射区 直接映射到物理内存
3. 8m空洞
4. vmalloc动态映射区
5. 永久映射区 允许建立长期映射关系
6. 固定映射区 虚拟地址固定 物理地址可变
7. 临时映射区 有时需要操作物理地址 但是内核不能直接操作物理地址 就可以映射到临时映射区

因为32位内核的虚拟地址空间有限 所以需要如此精细的管理 而64位无需如此复杂 很多都可以直接映射到物理内存
总线上传输的都是物理地址 内存一次吞吐8个byte（64位）cpu一次读写一个cache line（64byte）

## 深入理解linux物理内存管理

为了快速索引到具体的物理内存页 每个物理页的struct page结构体定义了一个索引编号 pfn
内核中如何管理这些物理内存页 struct page的方式被称为物理内存模型

### 从cpu角度看物理内存模型

#### flatmem 平坦内存模型

物理地址和物理内存都连续 用数组管理struct page即可 pfn为其下标
linux早期使用这种方式 因为这种方法简单高效

#### discontigmem 非连续内存模型

如果内存不连续 中间的空洞也得有页表 就会造成大量浪费 于是可以用链表 每个节点对应一段平坦内存  

#### sparsemem 稀疏内存模型

因为内核支持物理内存的热插拔 物理内存的不连续变为常态 上面的非连续内存模型的节点也可能不是平坦内存了 这时候就需要对连续内存更细粒度的管理（这时候连续的内存可能很小） 这时用于管理连续内存块的单元被称为section 在每个section中 还是用数组管理连续内存的 这种模型可以被用于所有内存布局的情况

#### 物理内存热插拔

在大规模集群中 尤其是云 为了实现集群资源的动态均衡 可以通过物理内存热插拔的功能实现集群机器物理内存容量的动态增减 还可以提高集群的高可用性（坏了马上换）
热插拔分为两个阶段

1. 物理热插拔 从物理上将内存插入 拔出 设计硬件和内存的支持
2. 逻辑热插拔 由内核中的内存管理子系统复杂 主要工作：动态上线启用刚加入的热内存 动态下线刚刚热拔出的内存

拔出的过程要复杂得多 比如即将拔出的物理内存已经为进程分配了物理页 这就需要把已经分配的内存页进行迁移  也就要更新虚拟内存与物理内存的对应关系 对于用户空间没什么问题 但是内核空间有一段直接映射区 这部分内存无法迁移
为了解决这个问题 内核会将物理内存页按页面是否可以迁移进行分类 分为不可迁移 可回收 可迁移 然后在可能被拔出的内存中只分配可迁移的页 这些信息在内存初始化的时候就会被设定

### 从cpu角度看物理内存架构

把物理内存看作一个整体 从cpu访问物理内存的角度看物理内存的架构 并从cpu与物理内存的相对位置变化来看不同物理内存架构对性能的影响

#### 一致性内存访问 uma

cpu与内存的交互通过总线
uma中 同一个cpu对所有内存的访问速度一致 也就是各个核心地位相等 smp（对称多处理器）
结构简单 但是核心多了以后 总线带宽会成为内存瓶颈

#### 非一致内存访问 numa

内存不再是一整片 而是被划分为一个一个内存节点 每个核心有自己的本地内存节点 cpu访问本地内存不需要经过总线 本地内存不足需要跨节点访问 速度较慢 核心还是可以访问到所有内存 只是速度有差异
cpu和其本地内存节点组成numa节点 cpu和cpu之间通过qpi点对点完成互联 在numa架构下 只能使用非连续和稀疏内存模型 而uma架构可以使用全部三种

#### numa的内存分配策略

大致有以下4种

1. MPOL_BIND 必须在绑定的节点内存分配 内存不够就swap
2. NPOL_INTERLEAVE 本地节点和远程节点均允许分配内存
3. NPOL_PREFERRED 优先指定节点分配 指定节点内存不足就选择离指定节点最近的
4. NPOL_LOCAL（默认）优先本地分配 当本地内存不足可在远程节点分配

一般将应用程序绑定到固定的核心可以提升性能

### 内核如何管理numa节点

numa架构和uma在内核中都是使用相同的数据结构来组织管理 内核会把uma当作只有一个节点的numa
内核使用了struct pglist——data 的数据结构来描述numa节点 在早期 内核使用单链表管理这些结构
后期 内核使用一个全局数组来管理

### numa节点的物理内存区域的划分  

不同的numa节点内存区域并不一样 只有第一个节点可以包含所有的区域 因为内核有直接映射区是从物理内存的起点开始的 还会有一个虚拟内存区 用来支持热插拔和处理碎片（类似swap）

### numa节点中的内存规整与回收

## 内核如何管理numa节点中的物理内存区域

内核中用来描述和管理numa节点中的物理内存区域的结构体是struct zone 在内核中访问非常频繁 内核将频繁访问的信息分成多个部分 然后加入一些填充 让这些部分能独占cache line 提升性能

### 物理内存区域中的预留内存

每个物理内存区域为操作系统预留了一部分内存 用于内核的一些核心操作（无论如何不允许内存分配失败的） 这些操作不允许阻塞 比如执行中断处理程序等
高位内存有时会对低位内存区域进行挤压 因为高位的内存空间一般是更通用的 当高位区域内存紧张 内核就会把内存分配到低位区域 但是这种挤占不能是无限制的 所以低位有自己的预留内存 预留内存的比例可以通过内核参数调整

### 物理内存区域的水位线

每个区域有自己的水位线 按每个区域的大小比例进行划分
有三条水位线 高阈值 低阈值 最低阈值 低于低阈值进行后台异步回收 直到高于高阈值 低于最低阈值启动直接回收  

#### 物理内存区域中的冷热页

热页就是已经加载进cpu cache中的物理内存页 而冷页就是还未加载进去的

在struct per_cpu_pages 结构中 每一种物理页的迁移类型都会对应一个冷热页链表

### 内核如何描述物理内存页

linux规定页的大小必须是2的整数次幂 这样可以提升运算效率 通常情况下 内存与磁盘间传输小块数据更加高效 所以默认4
k为页的大小
struct page需要被大量访问而且也大量存在 为了应付不同的情况 同时尽量减小大小 包含了大量的union

#### 反向映射

通常所说的内存映射是正向映射 从虚拟内存到物理内存的映射 反向映射则是反过来 用于当某个物理内存页需要进行回收或者迁移时找到其对应的虚拟内存 断开映射 如果没有反向映射就需要遍历
一个物理内存页可以映射到多个虚拟地址空间

#### 内存页回收相关属性  

用active和inactive两个链表管理 使用lru算法 匿名页被加载进内存会放在active尾部 文件页会被放在inactive头部（回收匿名页代价大一点） 为了提高查找效率 将文件页和匿名页用不同的链表存储  

#### 复合页 compound_page

对于一些内存敏感的使用场景中 用户往往期望一些巨型大页（通过多个连续的物理页组成） 这样就能减少缺页中断 提高效率还能减少页表项数量 提高缓存命中率 这样也减少了fork时的开销
linux仍是通过统一的struct page管理这些复合页 组成复合页的第一个物理页叫首页 其余称为尾页

#### slab对象池

有时不需要一下分配一页内存 这个时候就需要slab 

# 进程管理

## 进程 线程基础知识

### 进程

简而言之 就是运行的程序
多进程管理 需要中断 并发
大致有三种基本状态 阻塞 就绪 运行 还有两种是创建和结束 然后由于内存宝贵 程序可能被换出到硬盘 就称为挂起 挂起又分为阻塞挂起和就绪挂起
挂起不只是因为程序使用的空间不在内存 还可能是通过sleep暂时休眠 或者是用户通过crtl + z挂起了进程（linux）
操作系统通过pcb（进程控制块）描述进程 包括下面几个部分

1. 进程描述 包括唯一的标识符 和用户标识符
2. 进程控制与管理信息 标记进程状态和优先级
3. 资源分配清单 包含进程使用的地址空间 打开的文件列表
4. cpu相关信息 进程切换时 cpu寄存器的一些信息会被保留  

通常具有相同状态的进程会通过链表（进程状态经常变化 链表可以灵活的插入删除）组织在一起 形成队列
操作系统允许一个进程创建另一个进程 子进程会继承父进程的所有资源（拷贝）创建过程有如下三步

1. 申请空的pcb 向pcb填入信息
2. 为该进程分配资源
3. 将pcb加入就绪队列

进程有三种结束方式 正常结束 异常结束 外界干预（信号kill）
子进程结束后 从父进程继承的资源会还给父进程 父进程终止时 子进程会变为孤儿进程 会被一号进程收养
终止的过程

1. 查找对应的pcb
2. 如果处于执行状态 停止执行
3. 如果有子进程 把子进程交给一号进程
4. 将该进程的资源归还给操作系统
5. 删除其pcb

当进程需要等待某一事件完成 可以调用阻塞语句 一旦被阻塞 只能由另一个进程唤醒
阻塞的过程如下

1. 找到对应pcb
2. 如果正在运行 保存数据 转为阻塞态 停止运行
3. 将pcb插入阻塞队列

处于阻塞状态的进程需要别的进程唤醒 唤醒过程如下

1. 在阻塞队列中找到对应pcb
2. 把pcb从阻塞队列中移除 pcb置为就绪状态 放入就绪队列

cpu由运行一个进程切换到运行另一个进程叫上下文切换 由于涉及内核数据的保存与切换 需要在内核态完成

### 线程

轻量化的进程 可以并发运行 可以共享相同的地址空间（由同一个进程创建出来）
线程是进程的一条执行流程 为了保证线程控制流相对独立 每个线程有自己独立的寄存器和栈 其余的东西则是共享的
缺点 一个线程崩溃 同一个进程创建的所有线程都会崩溃（c/cpp有  java没有这个问题）
进程与线程的比较  

1. 进程是资源分配的单位 线程是cpu调度的单位
2. 进程拥有完整独立的资源 线程只独享必不可少的资源（栈和寄存器）
3. 进程同样具有运行 就绪 阻塞三种基本状态
4. 线程创建快 结束快 切换快 线程间数据交互也快

线程的上下文切换 操作系统的任务调度实际上是在调度线程

1. 如果切换的两线程属于一个进程 切换栈和寄存器即可
2. 不是同一个进程 就和进程切换一样

所以线程切换开销会更少

#### 线程的实现

主要有三种方法

1. 用户线程 在用户态实现 由用户态的线程库管理
2. 内核线程 由内核实现 由内核管理
3. 轻量级线程 在内核中支持用户线程

用户线程与内核线程也有三种

1. 多对一 多个用户线程对应一个内核线程
2. 一对一 一个用户线程对应一个内核线程
3. 多对多 多个用户线程对应多个内核线程

用户线程 基于用户态的线程管理库实现 tcb（线程控制块）在库里实现 操作系统看不见tcb 只能看见pcb 类似之前提到的多对一
优点

1. 可用于不支持线程的操作系统
2. 切换无需用户态与内核态的切换 速度快

缺点

1. 因为操作系统不参与 如果一个线程发起阻塞 整个进程都会被阻塞
2. 当一个线程开始运行时 出发主动交出cpu使用权 进程的其他线程都无法运行 因为操作系统才有打断当前运行线程的特权

内核线程 对应的tcb放在操作系统中 各种管理由操作系统负责 类似前面的一对一模型
优点

1. 一个线程被阻塞不影响该进程的其他线程

缺点

1. 需要内核维护tcb和pcb
2. 各种管理需要系统调用 开销较大

轻量级线程（lwp） 每个lwp与内核线程一一对应 由内核管理 像普通进程一样被调用 但是信息比进程少
lwp之上也可以使用用户线程 这时可以把lwp看作内核线程

### 调度


