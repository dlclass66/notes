# 零拷贝

因为磁盘速度很慢 使用零拷贝提高系统吞吐量 减少磁盘访问次数

## dma技术

没有dma之前 i/o 使用中断 由cpu控制数据的读取搬运
dma技术：在进行i/o设备和内存的数据传输时，数据搬运的事情全部交给dma控制器，cpu不再参与与数据搬运相关的事情 每个i/o设备有自己的dma控制器

## 传统的文件传输

一般需要read和write两个系统调用 发生了4次用户态和内核态的上下文切换 还发生了4次数据拷贝（2次dma拷贝， 两次cpu拷贝）

```c
read(file, tmp_buf, len)
//把内存缓冲区的文件读到用户缓冲区
write(socket, tmp_buf, len)
//把用户缓冲区的内容写到内存缓冲区
```

在高并发的情况下 冗余的上下文切换和数据拷贝严重影响性能

## 优化文件传输性能

要提高性能 就要减少上下文切换和内存拷贝
要减少前者就要减少系统调用的次数
减少后者可以通过取消用户缓冲区 （因为在用户态我们并不会对数据进行加工 所以可以不用搬到用户缓冲区）

## 如何实现零拷贝

通常有两种实现方式

1. mmap + write
2. sendfile

## 第一种

用mmap 替换read

```c
buf = mmap(file, len);
write(sockfd, buf, len);
```

mmap()会直接把内存缓冲区的数据映射到用户空间 这样内核与用户空间就不需要进行数据拷贝动作
这样做减少了一次数据拷贝（内核缓存拷贝到用户缓冲区的cpu拷贝） 这还不是最理想的零拷贝

## 第二种

专门发送文件的系统调用函数

```c
#include <sys/socket.h>
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
```

前两个参数分别是目的端和源端的文件描述符，后面两个参数是源端的偏移量和复制数据的长度 返回值是实际复制数据的长度
减少了一次系统调用 所以需要一次系统调用和三次数据拷贝 如果网卡支持 还可以将把数据从内存缓冲区拷贝到socket缓冲区的cpu拷贝也省去 这样就实现了cpu零拷贝
总体看来 零拷贝技术可以将文件传输性能提升一倍以上
nginx也支持零拷贝 一般默认开启

## pagecache

磁盘高速缓存 零拷贝使用了这一技术
由于局部性 可以用内存缓存最近访问的数据 当空间不足时淘汰最久未被访问的数据
读磁盘数据时 先在pagecache找 存在则直接返回 不存在则从磁盘读取然后缓存到pagecache中
pagecache还使用了预读功能 当在磁盘读数据时会将其相连的后面一部分数据也读到pagecache中
所以 pagecache的主要优点

1. 缓存最近被读取的数据
2. 预读
这样就大大提高了读写磁盘的性能

## 大文件

在传输大文件（gb级别）时，pagecache不会起作用，就白白浪费了dma做的内存拷贝，造成性能的降低， 零拷贝也就损失了性能
原因

1. pagecache很快被大文件占满
2. 其他热点小文件无法使用pagecache 磁盘读写性能下降
3. 大文件耗费dma拷贝到pagecache
所以大文件传输不应该使用pagecache 也就是说不应该使用零拷贝技术

## 大文件传输的实现方法

调用read进程会阻塞 这个问题可以用异步i/o处理 一般对于磁盘 异步i/o不能使用pagecache （绕过pagecache的叫做直接i/o）
所以 在高并发场景下 大文件传输应该使用异步i/o加直接i/o的方式代替零拷贝
直接i/o的常见应用场景

1. 应用程序已经实现了磁盘数据的缓存，可以不需要pagecache缓存
2. 传输大文件时

直接i/o无法享受内核的以下优化

1. i/o调度算法会缓存尽可能多的i/o请求在pagecache中 然后合并成一个大的请求
2. 预读

## 总结

大文件用异步i/o加直接i/o
小文件用零拷贝
零拷贝不能对文件进行进一步加工

# i/o多路复用

## 最简单的socket模型

socket是进程间通信比较特别的方式，可以跨主机通信 双方进行网络通信前 要各自都创建一个socket
创建socket时 可以指定网络层使用ipv4还是v6 传输层使用tcp还是udp
服务端socket编程过程（tcp）

1. 调用socket（）函数 创建网络协议为ipv4 传输协议为tcp的socket 接着使用bind 给socket绑定ip地址和端口号（通过端口号确定应用程序 通过ip地址确定要使用在多网卡情况使用哪个网卡）
2. 调用listen监听
3. 调用accept 从内核获取客户端的连接（阻塞）

客户端使用connect发起连接
服务器内核会为每个socket维护两个队列

1. 半连接队列 还没有完成三次握手的
2. 全连接队列 已经完成三次握手的
当全连接队列不为空 accept会从全连接队列中拿出一个socket返回应用程序 后续数据传输就使用这个socket

**注意** 监听的socket和真正用来传数据的socket不同
连接建立后 客户端和服务端就可以通过read和write来读写数据了
socket读写就像读写文件 内核中socket也是以文件的形式存在 有对应的文件描述符

## 文件描述符

每个进程有一个数据结构task_struct 该结构里有一个指向文件描述符数组的指针 该数组里有该进程打开的所有文件的文件描述符 数组的下标是文件描述符（int） 内容是一个指针 指向文件

## inode

每个文件有一个inode socket文件的inode指向内核中的socket结构 这个结构体里有两个队列 分别是发送队列和接收队列 队列中保存的是sk_buff结构体 用链表的形式组织
sk_buff结构体 可以保存各个层的数据包 避免了拷贝

## 如何服务更多的用户

前面的简单模型基本只能实现一对一通信（同步 阻塞）
理论单机服务最大的客户端数是客户端ip数 * 客户端端口数（因为tcp需要4元组 一般服务端都监听固定的ip和端口）
其他限制因素

1. 文件描述符 socket要有文件描述符 单进程打开的文件描述符有限制
2. 系统内存

## 多进程模型

比较传统的方式是为每一个客户端分配一个进程
服务器主进程监听客户连接 一旦完成 accept就返回一个已连接的socket 这时通过fork创建一个子进程 就把父进程所有相关的东西都复制给了子进程 根据返回值区分父子进程
因为子进程会复制父进程的文件描述符 就可以直接使用已连接socket和客户端通信了
当子进程退出时 父进程要回收僵尸进程的资源 使用wait（）和waitpid
客户端数量较多时 进程开销大

## 多线程模型

轻量级模型
单进程中可以运行多个进程，同进程里的线程可以共享部分资源 因此同一个进程下的线程上下文切换开销比进程小得多
当服务器与客户端连接成功后，通过pthread_create()创建线程，然后将已连接socket的文件描述符传递给线程函数，在线程中与客户端通信，达到了并发处理的目的
虽然线程创建销毁开销不大 但是大量并发下依然开销很大
使用线程池避免这些开销 原理 提前创建若干个线程 当新连接建立时 将已连接的socket放入一个队列中，由线程池中的线程负责从队列中取出已连接的socket进行处理
**注意** 这个队列是全局的（对于这个进程的所有线程来说）为了避免多线程竞争 需要加锁
但是即使这样 为每个连接分配一个线程的方案在高并发下依然无法完成

## i/o多路复用

使用一个进程维护多个socket
虽然一个进程在任一时刻只能处理一个请求，但是处理每个请求的事件时，耗时控制在1毫秒内，1秒就可以处理上千个请求，这就是多路复用，很类似cpu的并发 所以也叫时分多路复用
select/poll/epoll就是内核提供给用户的多路复用系统调用，进程可以通过一个系统调用从内核中获取多个事件
在获取事件时 先把所有连接（文件描述符）传给内核，再由内核返回产生了事件的连接，然后在用户态中处理这些连接对应的请求即可

## select/poll

select的实现方法 将已连接的socket放到一个文件描述符集合，然后调用select函数将文件描述符集合拷贝到内核中，让内核检查是否有事件产生，检查方式为遍历，检查到有事件发生时，将对应的socket标记为可读或可写，再把整个文件描述符集合拷贝到用户态，用户态通过遍历找到之前标记的socket
需要两次遍历和两次拷贝
select使用bitsmap表示文件描述符集合 有大小限制
poll使用动态数组，用链表组织，突破了select的大小限制
poll和select并没有本质区别 并发较高时 性能损耗都较大

## epoll

先用epoll_create 创建一个epoll对象 再用epoll_ctl将需要监视的socket添加到之前创建的epoll对象中 最后循环调用epoll_wait等待数据
epoll从下面两个方面解决了select/poll的问题

1. epoll在内核维护了一个红黑树跟踪进程所有待检测的文件描述符，降低了时间复杂度（降低到对数） 每次也只需要传入待检测的socket
2. epoll采用事件驱动的机制，内核中维护了一个链表记录就绪事件，当某个socket有事件发生时通过回调函数 内核会将其加入到就绪事件列表中，当用户调用epoll_wait时，只会返回有事件发生的文件描述符的个数，大大提高了效率
epoll_wait**仍然需要**将数据从内核态**拷贝**到用户态

## 边缘触发和水平触发

epoll支持两种触发模式，边缘触发（et）水平触发（lt）

* 边缘触发时，当socket上有可读事件时，服务端只会从epoll_wait苏醒一次，程序必须保证一次性将内核缓冲区的数据读取完
* 水平触发模式，服务端不断的苏醒，直到内核缓冲区数据被read函数读完才结束
水平触发只要满足事件的条件，就会一直不断把事件传给用户，而边缘触发只有第一次满足条件时才会通知用户
如果使用水平触发模式，当内核通知可读写时，没必要一次执行尽可能多的读写操作
使用边缘触发模式，一般和非阻塞i/o搭配使用，循环读取（如果阻塞，读完了就会卡住）直到系统调用返回错误（eagain， ewouldblock）
一般来说边缘触发效率更高，因为减少了系统调用次数
select/poll只能水平触发，epoll默认水平，但是可以设置边缘
多路复用api最好搭配非阻塞i/o使用，因为特殊情况可能会阻塞（比如api返回的socket并不是可读写的）

# 高性能网络模式 reactor和proactor

reactor被很多开源软件采用 比如redis nginx

## 演进

reactor是基于面向对象的思想，对i/o多路复用做了一层封装，让使用者无需考虑底层网络api的细节，只需要关注应用代码的编写
i/o多路复用监听事件，收到事件后，根据事件类型分配给某个进程或线程处理
reactor模式主要由reactor和处理资源池这两个核心部分组成

* reactor负责监听和分发事件，事件类型有连接事件，读写事件
* 处理资源池负责处理事件，如read->业务逻辑 ->send
reactor模式灵活多变，reactor可以有单个或多个，处理资源池也可以是单个或多个线程 排列组合就有4中方案，其中多reactor 单线程没有实际意义
剩下的三种都有广泛应用 使用线程还是进程则要看使用的编程语言和平台

## 单reactor 单线程

c语言一般是实现单进程，而java是单线程，因为java程序是跑在java虚拟机这个进程上面的
进程一般有三个对象

1. reactor 作用是监听和分发事件
2. acceptor 作用是获取连接
3. handler 作用是处理业务

工作流程

* reactor对象通过select监听事件，收到事件后 连接事件分发给acceptor处理，其他的分发给handler
* acceptor通过accept方法获取连接，并创建一个handler对象处理后续响应事件
* handler对象 read->业务处理 ->send 完成处理
好处 实现简单 无需进程间通讯， 没有多进程竞争
缺点

1. 无法充分利用多核cpu
2. handler处理业务时，整个进程无法处理其他连接，如果耗时较长就会造成延迟响应
所以这种模式不适用于计算密集的业务，只适合业务处理非常快的场景
redis 6.0之前对命令的处理就是这种方式

## 单reactor 多线程

流程

1. reactor通过select监听，收到后分发
2. 连接建立事件由acceptor处理，使用accept方法，创建handler对象处理后续的响应事件
3. 其余事件交由handler对象响应

以上和上一种方案一样 后面就不一样了

1. handler对象不再负责业务处理 只负责数据的接收发送，handler对象通过read读取到数据后，会将数据发给子线程里的processor对象进行业务处理
2. processor处理完后，将结果发给主线程的handler，handler通过send将响应发给客户

优点 充分利用多核cpu 缺点存在多线程竞争的问题
操作共享资源时需要加锁 多进程实现很麻烦 所以实际应用中没有多进程方案
单reactor的问题： 瞬间高并发时可能会成为瓶颈

## 多reactor 多进程

流程

1. 主线程中的mainreactor对象通过select监听，收到事件后通过acceptor accept 然后将新的连接分配给某个子线程
2. 子线程的subreactor将连接加入select继续监听，并创建一个handler处理连接的响应事件
3. 有新的事件发送，subreactor调用handler read->业务处理->send

看起来复杂 但是实现比上一种（单reactor， 多线程）简单 原因

1. 主线程 子线程分工明确 主线程只负责接收新连接
2. 主线程子线程交互简单，主线程只需要把新连接传给子线程， 子线程无需返回数据，直接将结果发给客户端即可

## proactor

reactor是非阻塞同步网络模式 而proactor是异步网络结构

### 阻塞i/o

read会等待内核数据准备好，然后把数据从内核态拷贝到用户态 最后才能返回
非阻塞 （socket设置）如果数据未准备好就直接返回（ewouldblock），继续往下执行，应用不断轮询，直到数据准备好，内核将数据拷贝到用户态，read调用才返回结果
无论阻塞还是非阻塞都是同步调用，因为最后把数据从内核拷贝到用户态都是要等待的 而异步不需要等待

### 异步i/o

性能更好

### proactor与reactor的区别

reactor感知可读写事件 感知到后需要应用程序主动读写 这个过程是同步的
proactor是异步的，感知的是已完成的读写事件，将缓冲区地址传入内核，读写工作由内核完成
linux下的异步i/o不完善 windows可以

# 一致性哈希

## 如何分配请求

大多数网站都不止一台服务器 而是服务器集群 这时就存在分配客户端请求的问题（负载均衡）
最简单的方法 引入一个负载均衡中间层 将外界请求轮流发给内部集群 简单的优化是根据节点硬件配置设置权重 实现加权轮询
但是上述都建立在每个服务器存储的数据相同的情况 这样无法应对分布式系统（数据分片）的场景

## hash算法的问题

简单的对于分布式系统的实现方法是hash（对节点数量取模） 对同一个关键词计算，每次得到的都是相同的值，这样就可以把key确定到一个节点了 但是一个致命的问题是系统扩容缩容时，一些数据的映射关系可能改变 需要迁移 最坏的情况是所有数据都要迁移 这样的成本显然太高

## 一致性hash算法的问题

一致性hash算法很好的解决了过多数据迁移的问题
一致性算法也是取模 但是不是对节点数量取模，而是固定对2^32取模可以把取模的结果组成一个圆环 称为hash环
一致性hash需要进行两步hash

1. 对存储节点进行hash计算（比如根据节点ip）
2. 当对数据进行存储或访问时，对数据进行hash映射
数据映射的结果的顺时针方向找到的第一个节点就是存储数据的节点
因此在增加或者移除一个节点时，仅会影响该节点在哈希环上顺时针相邻的后续节点，其他数据不会受影响
但问题是 一致性hash并不会保证节点的均匀分布，可能会有大量数据存储在一个节点上 而且在扩容或缩容时 相邻节点可能收到较大影响 导致雪崩的连锁反应

## 如何通过虚拟节点提高均衡度

如果有大量的节点 节点数越多，hash环上的节点的分布也就越均匀
实际没有那么多节点，我们就引入虚拟节点，也就是对一个真实节点做多个副本
具体做法是 不再将真实节点映射到hash环上，而是将虚拟节点映射到hash环上，并且将虚拟节点映射到实际节点，实际工程中，虚拟节点的数量可能有数百个（每个真实节点）
虚拟节点除了提高了节点的均衡度，还提升了系统的稳定度，当节点变化时，会有不同的节点分担
而不同硬件性能的节点可以有不同数量的虚拟节点（代表权重）
